{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression and Regularization\n",
    "\n",
    "This notebook will review the slides for linear regression, with examples.\n",
    "It will also show examples of different regularizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Review model (derivation too?)\n",
    "2. Simple example\n",
    "3. Review regularization\n",
    "4. Simple examples (change regularizer, look at coefficients and results?\n",
    "\n",
    "What's a good data set to use for these examples?  \n",
    "Things that sparsity works good for?  \n",
    "What if we already have enough material, and we really only need the logistic regression notebook?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Models\n",
    "\n",
    "Many problems in machine learing seek to build a model\n",
    "\n",
    "$$g(a; x) \\approx y$$\n",
    "\n",
    "given a data set\n",
    "\n",
    "$$\\{(a_1, y_1), \\dots, (a_m, y_m)\\},$$\n",
    "\n",
    "with components\n",
    "* $a_i \\in \\mathbf{R}^n$ - data features,\n",
    "* $y_i \\in \\mathbf{R}$ or $\\{0, 1\\}$ - data value or class,\n",
    "* $g: \\mathbf{R}^n \\to \\mathbf{R}$ or $\\{0, 1\\}$ - prediction function,\n",
    "* $x \\in \\mathbf{R}^n$ - model parameters,\n",
    "* $m$ - number of data points, and\n",
    "* $n$ - number of data features.\n",
    "\n",
    "We can fit a model to the given data by solving an optimization problem of the form\n",
    "\n",
    "$$\\min_x \\sum_{i=1}^m f_i(g(a_i; x), y_i) + r(x)$$\n",
    "\n",
    "with components\n",
    "* $x \\in \\mathbf{R}^n$ - model parameters,\n",
    "* $f_i: \\mathbf{R}^n \\to \\mathbf{R}$ - functions that measure how well the model fits the data for a given set of parameters, and\n",
    "* $r(x): \\mathbf{R}^n \\to \\mathbf{R}$ - regularization function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "In the linear regression problem, we would like to find a linear predictor\n",
    "\n",
    "$$g(a_i; x) = x_1 a_{i1} + \\dots + x_n a_{in} = a_i^T x \\approx y_i,$$\n",
    "\n",
    "where both $a_i$ and $y_i$ are continuous.\n",
    "One approach for deriving the functions $f_i$ is to assume a statistical model for the error in the data set, and then develop a [maximum likelihood](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) formulation.\n",
    "Here we will assume that errors are independently drawn from a normal distribution with mean zero and variance $\\sigma^2$, that is, \n",
    "\n",
    "$$y_i = a_i^Tx + \\epsilon_i, \\quad \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2).$$\n",
    "\n",
    "This means that the probability density function for an observation $(a_i, y_i)$ given model parameters $x$ is\n",
    "\n",
    "$$p((a_i, y_i); x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - a_i^Tx)^2}{2\\sigma^2}\\right),$$\n",
    "\n",
    "and the PDF of all $m$ i.i.d. observations is\n",
    "\n",
    "\\begin{align}\n",
    "p\\big(\\{(a_1, y_1), \\dots, (a_m, y_m)\\}; x\\big) &= \\prod_{i=1}^m p((a_i, y_i); x) \\\\\n",
    "&= \\prod_{i=1}^m \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - a_i^Tx)^2}{2\\sigma^2}\\right) \\\\\n",
    "&= \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{m/2} \\exp\\left(-\\frac{\\sum_{i=1}^m (y_i - a_i^Tx)^2}{2\\sigma^2}\\right).\n",
    "\\end{align}\n",
    "\n",
    "Alternatively, we can consider the likelihood of a set of model parameters given our $m$ observations, \n",
    "\n",
    "$$\\mathcal{L}\\big(x; \\{(a_1, y_1), \\dots, (a_m, y_m)\\}\\big) = p\\big(\\{(a_1, y_1), \\dots, (a_m, y_m)\\}; x\\big),$$\n",
    "\n",
    "so that we can solve an optimization problem to find the parameters with the maximum likelihood. In practice, this is often done by minimizing the negative log-likelihood, which, ignoring the constant coefficient, results in our least-squares problem\n",
    "\n",
    "$$\\min_x \\sum_{i=1}^m(y_i - a_i^Tx)^2 = \\|y - Ax\\|^2.$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
